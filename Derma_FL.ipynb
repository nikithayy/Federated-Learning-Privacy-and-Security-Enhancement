{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda84d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Import Statements\n",
    "#----------------------\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from skimage import restoration, img_as_float\n",
    "from skimage.color import rgba2rgb\n",
    "from PIL import Image\n",
    "\n",
    "# Torch stuff (for model part)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc016b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Variables Declarations\n",
    "#------------------------\n",
    "medmnist_dir = \"../Dataset_Derma/data_raw\"                     # Directory for raw DermaMNSIT Data\n",
    "preprocess_dir = \"../Dataset_Derma/data_preprocessed\"          # Base Directory for preprocessed Data\n",
    "clean_output_dir = os.path.join(preprocess_dir, \"dermanist_clean\") # Directory to Store Clean Clients data and labels\n",
    "poisoned_output_dir = os.path.join(preprocess_dir, \"dermanist_poison_flip\")  # Directory to Store Poisoned Clients data and labels\n",
    "\n",
    "# To check if the directories exist in the system\n",
    "os.makedirs(medmnist_dir, exist_ok=True)\n",
    "os.makedirs(preprocess_dir, exist_ok=True)\n",
    "\n",
    "# Processing / client partition options\n",
    "NUM_CLIENTS = 20          # No.of Federated Clients\n",
    "RANDOM_SEED = 42           \n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Poisoning config\n",
    "POISONED_CLIENT_NUM = 4           # No.of Clients to Corrupt as per our Group no:9       \n",
    "FLIPPING_RATES = [0.1, 0.25, 0.5, 0.75]  # mapped to poisoned clients\n",
    "\n",
    "# -------------------------\n",
    "# Training Config\n",
    "# -------------------------\n",
    "NUM_CLASSES = 7   # No.of CLasses as per MEDMNIST Site\n",
    "BATCH_SIZE = 64   # Batch Size for Dataloading\n",
    "IMAGE_SIZE = 28   # Image Dimensions (28x28 pixels)\n",
    "EPOCHS = 10       # No.of Epochs\n",
    "LR = 0.001             # Learning Rate (Fine-tune)\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "#File to save Pretrained initial model\n",
    "INITIAL_MODEL_PRETRAINED = \"initial_resnet18_pretrained.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba2eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line if this is the first time and didn't install medmnist dataset\n",
    "# %pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a22ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19.7M/19.7M [00:06<00:00, 3.17MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 7007\n",
      "Val samples:   1003\n",
      "Test samples:  2005\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Section B: Load medmnist DermaMNIST Dataset\n",
    "# -------------------------\n",
    "\n",
    "# Attempt to import the MedMNIST (DermaMNIST in Particular)\n",
    "try:\n",
    "    import medmnist\n",
    "    from medmnist import DermaMNIST\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install medmnist: pip install medmnist by uncommenting the cell before\") from e\n",
    "\n",
    "# ensure raw data dir exists\n",
    "os.makedirs(medmnist_dir, exist_ok=True)\n",
    "\n",
    "# Load Train, Validation and test Splits from DermaMNIST\n",
    "derma_train = DermaMNIST(root=medmnist_dir, split=\"train\", download=True, size=IMAGE_SIZE)\n",
    "derma_val   = DermaMNIST(root=medmnist_dir, split=\"val\", download=True, size=IMAGE_SIZE)\n",
    "derma_test  = DermaMNIST(root=medmnist_dir, split=\"test\", download=True, size=IMAGE_SIZE)\n",
    "\n",
    "# Printing those samples count/size\n",
    "print(\"Train samples:\", len(derma_train))\n",
    "print(\"Val samples:  \", len(derma_val))\n",
    "print(\"Test samples: \", len(derma_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7eb1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates Removed:  1\n",
      "Kept 7006 unique & denoised images from train split\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1) Data Cleaning:\n",
    "#    Preprocess: deduplicate + denoise + channel normalize\n",
    "# -------------------------\n",
    "\n",
    "# Compute SHA-1 hash of image to detect duplicates\n",
    "def sha1_of_image_uint8(img_uint8: np.ndarray) -> str:\n",
    "    return hashlib.sha1(img_uint8.tobytes()).hexdigest()\n",
    "\n",
    "seen_hash = set()\n",
    "processed: List[Dict] = []\n",
    "count=0\n",
    "\n",
    "# Loop through train dataset\n",
    "for i in range(len(derma_train)):\n",
    "    img_pil, label = derma_train[i]\n",
    "    label = int(label.item())\n",
    "    img = np.array(img_pil)  # PIL -> numpy\n",
    "\n",
    "    # ensure 3 channels (uint8)\n",
    "    if img.ndim == 2:\n",
    "        img = np.stack([img]*3, axis=-1)\n",
    "    elif img.shape[2] == 4:\n",
    "        # rgba -> rgb (returns float in [0,1] for rgba2rgb if input normalized; but it handles uint8 as well)\n",
    "        img = (rgba2rgb(img / 255.0) * 255).astype(np.uint8)\n",
    "\n",
    "    img_float = img_as_float(img)  # Normalize to float [0,1]\n",
    "    img_uint8 = (np.clip(img_float, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "    # check for Duplicates\n",
    "    img_hash = sha1_of_image_uint8(img_uint8)\n",
    "    if img_hash in seen_hash:\n",
    "        count+=1\n",
    "        print(\"Duplicates Removed: \",count)   #To check how many Duplicates are removed\n",
    "        continue\n",
    "    seen_hash.add(img_hash)\n",
    "\n",
    "    # denoise (fallback to original if failed)\n",
    "    try:\n",
    "        denoise = restoration.denoise_bilateral(img_float, channel_axis=-1)\n",
    "    except Exception:\n",
    "        denoise = img_float\n",
    "\n",
    "    denoise_uint8 = (np.clip(denoise, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "    processed.append({\n",
    "        \"image\": denoise_uint8,\n",
    "        \"label\": label,\n",
    "        \"orig_index\": i\n",
    "    })\n",
    "\n",
    "print(f\"Kept {len(processed)} unique & denoised images from train split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dbfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2 & 3) Data Normalization and Augmentation\n",
    "#     Define transforms\n",
    "#    - save_transform for saving augmented PNGs\n",
    "#    - model transforms for training/eval\n",
    "# -------------------------\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# Augmentation transforms for saving images\n",
    "save_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.2,0.2,0.2)], p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Training transform (resize, normalize)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Validation or Test transform (resize, normalize)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# -------- For pretrained ResNet-18 (ImageNet normalization) --------\n",
    "pretrain_train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "pretrain_eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9664f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client sample counts: [38, 163, 865, 207, 140, 810, 305, 1351, 102, 357, 194, 86, 168, 101, 1166, 177, 85, 137, 193, 361]\n",
      "Total samples: 7006\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 4) Data Partitioning: Partition into clients (non-IID via Dirichlet)\n",
    "# -------------------------\n",
    "labels = np.array([r[\"label\"] for r in processed])\n",
    "num_classes_in_data = labels.max() + 1\n",
    "assert num_classes_in_data == NUM_CLASSES or NUM_CLASSES == 7, \"Verify NUM_CLASSES\"\n",
    "\n",
    "# create index list by label\n",
    "idx_by_label = [np.where(labels == class_id)[0].tolist() for class_id in range(num_classes_in_data)]\n",
    "\n",
    "client_index = [[] for _ in range(NUM_CLIENTS)]\n",
    "\n",
    "# Distribute samples per class across clients using Dirichlet distribution\n",
    "for class_id, idxs in enumerate(idx_by_label):\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "    np.random.shuffle(idxs)\n",
    "    proportions = np.random.dirichlet([0.5] * NUM_CLIENTS)\n",
    "    splits = (np.cumsum(proportions) * len(idxs)).astype(int)\n",
    "    prev = 0\n",
    "    for cid, cut in enumerate(splits):\n",
    "        client_index[cid].extend(idxs[prev:cut])\n",
    "        prev = cut\n",
    "    client_index[-1].extend(idxs[prev:])\n",
    "\n",
    "# Build per-client dataset dictionary\n",
    "client_data = {cid: [processed[i] for i in client_index[cid]] for cid in range(NUM_CLIENTS)}\n",
    "print(\"Client sample counts:\", [len(v) for v in client_data.values()])\n",
    "print(\"Total samples:\", sum(len(v) for v in client_data.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38ab8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean client datasets as PNG images. Total images: 7006\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5) Data Labelling: Save CLEAN per-client datasets (augmented images saved)\n",
    "# -------------------------\n",
    "os.makedirs(clean_output_dir, exist_ok=True)\n",
    "total_images = 0\n",
    "\n",
    "for cid, recs in client_data.items():\n",
    "    client_dir = os.path.join(clean_output_dir, f\"Client_{cid}\")\n",
    "    img_dir = os.path.join(client_dir, \"images\")\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    for idx, rec in enumerate(recs):\n",
    "        img = rec[\"image\"]  # uint8 HxWx3\n",
    "        label = rec[\"label\"]\n",
    "\n",
    "        # Apply augmentation pipeline before saving (use save_transform which expects HxWxC uint8)\n",
    "        # It returns tensor; convert back to uint8 HWC for PNG.\n",
    "        img_tensor = save_transform(img)  # C x H x W, float [0,1]\n",
    "        img_aug = (img_tensor.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "\n",
    "        filename = f\"{idx}.png\"\n",
    "        filepath = os.path.join(img_dir, filename)\n",
    "        imageio.imwrite(filepath, img_aug)\n",
    "        records.append({\"filename\": filename, \"label\": label})\n",
    "        total_images += 1\n",
    "\n",
    "    # Save CSV with Labels\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(os.path.join(client_dir, \"labels.csv\"), index=False)\n",
    "\n",
    "print(f\"Saved clean client datasets as PNG images. Total images: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac65fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved poisoned datasets. Total poisoned images (all clients): 7006\n",
      "Poisoned clients and flip map: {1: 0.1, 12: 0.25, 15: 0.5, 2: 0.75}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Data Corrpution: Create POISONED datasets (label flipping)\n",
    "# -------------------------\n",
    "os.makedirs(poisoned_output_dir, exist_ok=True)\n",
    "rng = np.random.RandomState(RANDOM_SEED + 9)  \n",
    "\n",
    "# Select CLients for poisioning\n",
    "poisoned_client_ids = rng.choice(NUM_CLIENTS, POISONED_CLIENT_NUM, replace=False).tolist()\n",
    "client_flip_map = dict(zip(poisoned_client_ids, FLIPPING_RATES[:POISONED_CLIENT_NUM]))\n",
    "\n",
    "client_data_corrupted = {}\n",
    "corruption_summary = {}\n",
    "\n",
    "# Apply Label Flipping\n",
    "for cid, recs in client_data.items():\n",
    "    recs_copy = [dict(r) for r in recs]  # Copy Records\n",
    "    flipped_count = 0\n",
    "    total = len(recs_copy)\n",
    "    if cid in client_flip_map:\n",
    "        flip = client_flip_map[cid]\n",
    "        n = len(recs_copy)\n",
    "        num_to_flip = int(round(flip * n))\n",
    "        if num_to_flip > 0:\n",
    "            flip_idx = rng.choice(n, num_to_flip, replace=False)\n",
    "            for idx in flip_idx:\n",
    "                orig = recs_copy[idx][\"label\"]\n",
    "                choices = [l for l in range(NUM_CLASSES) if l != orig]\n",
    "                new_label = int(rng.choice(choices))   # Random Wrong Lbel\n",
    "                recs_copy[idx][\"label\"] = new_label\n",
    "                flipped_count += 1\n",
    "\n",
    "    client_data_corrupted[cid] = recs_copy\n",
    "    corruption_summary[cid] = {\n",
    "        \"total\": total,\n",
    "        \"flipped\": flipped_count,\n",
    "        \"percent\": 100 * flipped_count / total if total > 0 else 0\n",
    "    }\n",
    "\n",
    "# Save poisoned clients images and labels\n",
    "total_poisoned_images = 0\n",
    "for cid, recs in client_data_corrupted.items():\n",
    "    if not recs:\n",
    "        continue\n",
    "    client_dir = os.path.join(poisoned_output_dir, f\"Client_{cid}\")\n",
    "    img_dir = os.path.join(client_dir, \"images\")\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    records = []\n",
    "    for idx, rec in enumerate(recs):\n",
    "        img = rec[\"image\"]\n",
    "        label = rec[\"label\"]\n",
    "        filename = f\"{idx}.png\"\n",
    "        filepath = os.path.join(img_dir, filename)\n",
    "        imageio.imwrite(filepath, img)\n",
    "        records.append({\"filename\": filename, \"label\": label})\n",
    "        total_poisoned_images += 1\n",
    "    pd.DataFrame(records).to_csv(os.path.join(client_dir, \"labels.csv\"), index=False)\n",
    "\n",
    "meta = {\n",
    "    \"poisoned_client_ids\": poisoned_client_ids,\n",
    "    \"client_flip_map\": client_flip_map,\n",
    "    \"seed\": RANDOM_SEED,\n",
    "    \"corruption_summary\": corruption_summary\n",
    "}\n",
    "with open(os.path.join(poisoned_output_dir, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved poisoned datasets. Total poisoned images (all clients): {total_poisoned_images}\")\n",
    "print(\"Poisoned clients and flip map:\", client_flip_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aee5a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total centralized training samples: 7006\n",
      "Train: 7006 | Val: 1003 | Test: 2005\n",
      "Epoch 1/10 | Loss: 0.9933 | Train Acc: 0.6643 | Val Acc: 0.7009 | Val AUC: 0.8443\n",
      "Saved best model at epoch 1 (Val Acc: 0.7009)\n",
      "Epoch 2/10 | Loss: 0.8467 | Train Acc: 0.6984 | Val Acc: 0.7089 | Val AUC: 0.8699\n",
      "Saved best model at epoch 2 (Val Acc: 0.7089)\n",
      "Epoch 3/10 | Loss: 0.7936 | Train Acc: 0.7140 | Val Acc: 0.7408 | Val AUC: 0.8955\n",
      "Saved best model at epoch 3 (Val Acc: 0.7408)\n",
      "Epoch 4/10 | Loss: 0.7526 | Train Acc: 0.7235 | Val Acc: 0.7109 | Val AUC: 0.8956\n",
      "Epoch 5/10 | Loss: 0.7177 | Train Acc: 0.7368 | Val Acc: 0.6959 | Val AUC: 0.8995\n",
      "Epoch 6/10 | Loss: 0.7005 | Train Acc: 0.7398 | Val Acc: 0.6810 | Val AUC: 0.9020\n",
      "Epoch 7/10 | Loss: 0.6786 | Train Acc: 0.7439 | Val Acc: 0.7168 | Val AUC: 0.9005\n",
      "Epoch 8/10 | Loss: 0.6631 | Train Acc: 0.7564 | Val Acc: 0.6650 | Val AUC: 0.9081\n",
      "Epoch 9/10 | Loss: 0.6440 | Train Acc: 0.7581 | Val Acc: 0.6550 | Val AUC: 0.8963\n",
      "Epoch 10/10 | Loss: 0.6358 | Train Acc: 0.7608 | Val Acc: 0.7079 | Val AUC: 0.9046\n",
      "\n",
      " Final Test Results (Pretrained) → ACC: 0.7157, AUC: 0.8823\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Section C: Using a Pre-trained Model (ResNet-18)\n",
    "# =========================================================\n",
    "\n",
    "# -------------------------\n",
    "# Dataset class\n",
    "# -------------------------\n",
    "class SimpleImageDataset(Dataset):\n",
    "    '''Reads Images and Labels from CSV'''\n",
    "    def __init__(self, csv_file_or_df, root_dir=None, transform=None):\n",
    "        if isinstance(csv_file_or_df, pd.DataFrame):\n",
    "            self.df = csv_file_or_df.reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv_file_or_df)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = str(row[\"filename\"])\n",
    "        if os.path.isabs(filename):\n",
    "            img_path = filename\n",
    "        else:\n",
    "            img_path = os.path.join(self.root_dir, filename) if self.root_dir else filename\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(row[\"label\"])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Select Pre-trained Model\n",
    "# -------------------------\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Adapting the Model\n",
    "# -------------------------\n",
    "# Adjusted conv1 and remove maxpool to fit small 28x28 inputs\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "# Replaced final FC layer\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Fine-tuning\n",
    "# -------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Build datasets using PRETRAIN transforms\n",
    "# Collect training samples from all clients (On Centralised Dataset)\n",
    "train_rows = []\n",
    "for client_name in os.listdir(clean_output_dir):\n",
    "    client_dir = os.path.join(clean_output_dir, client_name)\n",
    "    labels_csv = os.path.join(client_dir, \"labels.csv\")\n",
    "    img_dir = os.path.join(client_dir, \"images\")\n",
    "    if os.path.exists(labels_csv):\n",
    "        df = pd.read_csv(labels_csv)\n",
    "        for _, r in df.iterrows():\n",
    "            abs_path = os.path.abspath(os.path.join(img_dir, r[\"filename\"]))\n",
    "            train_rows.append({\"filename\": abs_path, \"label\": int(r[\"label\"])})\n",
    "\n",
    "train_df = pd.DataFrame(train_rows)\n",
    "print(\"Total centralized training samples:\", len(train_df))\n",
    "if len(train_df) == 0:\n",
    "    raise RuntimeError(\"No centralized training samples found in clean_output_dir\")\n",
    "\n",
    "# Use centralized dataset + MedMNIST val/test\n",
    "train_dataset = SimpleImageDataset(train_df, transform=pretrain_train_transform)\n",
    "val_dataset   = DermaMNIST(root=medmnist_dir, split=\"val\",  download=True, transform=pretrain_eval_transform)\n",
    "test_dataset  = DermaMNIST(root=medmnist_dir, split=\"test\", download=True, transform=pretrain_eval_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, preds_train, labels_train = 0.0, [], []\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds_train.extend(outputs.argmax(1).cpu().numpy())\n",
    "        labels_train.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(labels_train, preds_train)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            y_probs.extend(probs)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_probs = np.array(y_probs)\n",
    "    preds = y_probs.argmax(axis=1)\n",
    "    val_acc = accuracy_score(y_true, preds)\n",
    "    try:\n",
    "        val_auc = roc_auc_score(y_true, y_probs, multi_class=\"ovr\")\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {train_loss:.4f} \"\n",
    "          f\"| Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), INITIAL_MODEL_PRETRAINED)\n",
    "        print(f\"Saved best model at epoch {epoch+1} (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "# -------------------------\n",
    "# Final Test Evaluation\n",
    "# -------------------------\n",
    "model.load_state_dict(torch.load(INITIAL_MODEL_PRETRAINED))\n",
    "model.eval()\n",
    "y_true, y_probs = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        y_probs.extend(probs)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_probs = np.array(y_probs)\n",
    "preds = y_probs.argmax(axis=1)\n",
    "test_acc = accuracy_score(y_true, preds)\n",
    "try:\n",
    "    test_auc = roc_auc_score(y_true, y_probs, multi_class=\"ovr\")\n",
    "except:\n",
    "    test_auc = float(\"nan\")\n",
    "\n",
    "print(f\"\\n Final Test Results (Pretrained) → ACC: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
